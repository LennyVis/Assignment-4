---
title: "Assignment 1 - MA710 Bentley"
output: html_document
---

## 1. Introduction

The data we are working on is provided by the U.S department of Education. The data set contains information of College Scorecard, federal financial aid and students' earning records. The data set can help to understand the information of getting federal financial and have insights on the school performance as well as students' long term development. The data set contains 1743 variables which are organized into 10 categories. The categories cover a range of topics:

 * Root - basic information such as School Id and location.

 * School - basic descriptive information such as degree types, number of branches, Carnegie classification, Public or Private etc.

 * Academics - types of academics available at the school.

 * Admissions - admission statistics such as acceptance rate and SAT/ACT scores.

 * Student - descriptions of the students including the number of undergraduate students, part-time or full-time students, share of first generation students.

 * Cost - information  cost for students such as average cost of attendance, tuition and fees, and average net price for institutions by income level.

 * Aid - the amount of debt a student can take on while attending the college, typical monthly loan payments, and the percentage of Pell Grant Recipients.

 * Repayment - descriptions of how successful students are at repaying their debts which is measured by cohort default rate and repayment rate.

 * Completion - information on the completion and retention rates for first time and full time students.

 * Earnings - statistics on average and median earnings as well as share of former students' earning more than $25,000. 


Based on the information provided by the data set,the descriptive analysis is performed. 


## 2.Goals
"Private colleges vs. public colleges". There is a face-off that is been ongoing for many years. The major difference between public and private colleges lies in how they are funded. Public colleges are funded mostly by state governments, whereas private colleges rely heavily on private contributions, donations and tuition. The goal of our analysis is to build a model which predicts whether an institution is public or private based on a numer of variables describing the colleges. The variables we will use include percentage of first time college graduates, percentage of female students, types of degree awarded, annual cost of attendance.  


## 3.DataSet Description

There are 26 variables that we are interested in. The variables name,  type and descriptions are listed below. 

*For variable `NPT4_PRIV` and `NPT4_PUB`, they includes tuition, supplies, living expenses minus the average grand or scholarship; the cost is calculated for full-time, fist-time, degree and certificate seeking undergraduates


|Variables        |Type     |Descriptions|
|-----------------|---------|-------------------------------
|Type             |FACTOR     |Public or Private Institution |
|Family_Inc       |NUMERIC    |Average family income in real 2015 dollars ($)|
|Female           |NUMERIC    |Percentage of female students|
|Married          |NUMERIC    |Percentage of married students|
|DEP_STAT_PCT_IND |NUMERIC    |Percentage of financially independent students|
|First_Gen        |NUMERIC    |Percentage of first generation students|
|NPT4_PRIV        |NUMERIC    |Average annual cost of attendance a Private college ($)|
|NPT4_PUB         |NUMERIC    |Average annual cost of attendance a Public college ($)|
|NUMBRANCH        |NUMERIC    |Number of Branch Campuses|
|PCIP01           |NUMERIC    |Percentage of degrees awarded in Agriculture , Agriculture Operations, And Related Sciences|
|PCIP03           |NUMERIC    |Percentage of degrees awarded in Natural Resources And Conservation|
|PCIP04           |NUMERIC    |Percentage of degrees awarded in Architecture And Related Services|
|PCIP05           |NUMERIC    |Percentage of degrees awarded in Area, Ethnic, Cultural, Gender, And Group Studies|
|PCIP09           |NUMERIC    |Percentage of degrees awarded in Communication, Journalism, And Related Programs|
|PCIP10           |NUMERIC    |Percentage of degrees awarded in Communications Technologies/Technicians And Support Services|
|CDR3             |NUMERIC    |Three-year cohort default rate|
|CDR3_DENOM       |NUMERIC    |Number of students in the cohort for the three-year cohort default rate|
|PCTPELL          |NUMERIC    |Percentage of undergraduates who receive a Pell Grant|
|PCTFLOAN         |NUMERIC    |Percent of all federal undergraduate students receiving a federal student loan|
|DEBT_N           |NUMERIC    |The number of students in the median debt completers cohort|
|CUML_DEBT_N      |NUMERIC    |Number of students in the cumulative loan debt cohort|



## 4.Objectives

Our main goal of the project is to predict whether an institution is public or private. Based on this goal, we will define a number of objectives:


* Apply k neighbour and regression tree model algorithms for predicting the type of institution
* Fine tune the parameters of each of the algorithms in order to improve the prediction power
* Compare the results between the two algorithms
* Investigate which of the used variables are more important in determining the type of college


## 5.Data Preparation

Before applying both predictive algorithms, we will perform some data cleaning and preparation.

### 5.1 Loading libraries

We will load the `dplyr` package that contains functions for data manipulation using data frames. It allows us to order rows, select rows, select variables, modify variables and summarize variables. We will also load the `ggplot2` package which is a powerful plotting package that creates elegant and complex plots in R. The `magrittr` library is loaded for the piping operator `%>%`. The `readr` library is loaded to read in the csv file.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r,message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(magrittr)
library(readr)
library(caTools)
```

Then we read in the csv file `MERGED2014_15_PP.csv` into the data frame `csb.df` using the `read_csv` function of the `readr` package.

```{r,message=FALSE, warning=FALSE}
csb.df = read_csv(paste0('C:/Users/sevda/Documents/Bentley/710/Assignment 1/CollegeScorecard_Raw_Data/CollegeScorecard_Raw_Data/', 'MERGED2014_15_PP.csv'))

```
### 5.2 Data cleaning and variable modification

For our analysis, we wish to include only those institutions which are currently operating. So we filter the rows to keep those which have `CURROPER` equal to 1.

```{r,message=FALSE, warning=FALSE}
csb.df %>%
  filter(CURROPER ==1) %>%
  {.} -> csbop.df
```
The new data frame `csbop.df` consists of only currently operating institutions.
Based on our decision to select certain variables, we will use the `SELECT` command from `dply` package to create a smaller data frame which contains only the variables of interest.

```{r,message=FALSE, warning=FALSE}
csbop.df %>%
  select(CONTROL, FAMINC, FEMALE, MARRIED,DEP_STAT_PCT_IND, FIRST_GEN, NPT4_PRIV, NPT4_PUB, NUMBRANCH, 
         PCIP01,PCIP03,PCIP04,PCIP05,PCIP09,PCIP10, CDR3, CDR3_DENOM, PCTPELL, PCTFLOAN,
          DEBT_N, CUML_DEBT_N) %>%
  {.} -> csb.vars.df
```
The resulting data frame `csb.vars.df` contains only the selected variables. Next we get a glimpse of this new data frame `csb.vars.df`. The `glimpse` function allows us to see all the columns in the data frame.

```{r,message=FALSE, warning=FALSE}
glimpse(csb.vars.df)
```

The output tells us that the data frame `csb.vars.df` has 6893 observations and 27 variables. It also displays the class of these variables.We can see that some of the variables do no belong to the correct class. For instance, `CONTROL`(Type of institution - Public or Private) should be a factor variable, but it is an integer in the data frame. Similarly FEMALE (Percentage of female students) should be an integer, and not a character variable.

To convert the class of these variables to the desired class, we use the functions `as.numeric` and `as.factor`, which convert the class variables to `numeric` and `factor` respectively.The code below uses the `MUTATE` command from the `dplyr` library to alter variables of the input data frame `csb.vars.df` and returns the modified data frame `csb.class.df`. 

```{r,message=FALSE, warning=FALSE}
csb.vars.df %>% 
  mutate(
         CONTROL    = as.factor(CONTROL),
         FAMINC  = as.numeric(FAMINC),
         FEMALE = as.numeric(FEMALE), 
         MARRIED  = as.numeric(MARRIED),
         DEP_STAT_PCT_IND = as.numeric(DEP_STAT_PCT_IND),
         FIRST_GEN = as.numeric(FIRST_GEN),
         NPT4_PRIV = as.numeric(NPT4_PRIV),
         NPT4_PUB = as.numeric(NPT4_PUB),
         NUMBRANCH = as.numeric(NUMBRANCH),
         PCIP01=as.numeric(PCIP01),
         PCIP03=as.numeric(PCIP03),
         PCIP04=as.numeric(PCIP04),
         PCIP05=as.numeric(PCIP05),
         PCIP09=as.numeric(PCIP09),
         PCIP10=as.numeric(PCIP10),
         CDR3 = as.numeric(CDR3), 
         CDR3_DENOM  = as.numeric(CDR3_DENOM),
         PCTPELL = as.numeric(PCTPELL),
         PCTFLOAN = as.numeric(PCTFLOAN),
         DEBT_N = as.numeric(DEBT_N),
         CUML_DEBT_N = as.numeric(CUML_DEBT_N)) %>%
         {.} -> csb.var.df
```

The next step is to check for the percentage of NA (missing) values for the above range of variables. This is accomplished by first adding the number of NA values for each variable, dividing it by the `length` of the variable(total number of observations) and multiplying by 100 to convert it into percentage. 

Since we would like to apply this function to a range of variables, we use `summarise_at` command. `summarise_at` takes the set of columns to be summarized as the first parameter which in our case wold be all columns and the function to be applied to them as the second parameter.

```{r,message=FALSE, warning=FALSE}
csb.var.df %>% 
  summarise_at(.cols = vars(),
               .funs = function(x) sum(is.na(x))/length(x))*100
```

We can observe from the output that `FAMINC` has 4.16% missing values, `MARRIED` has 15.32% missing values and so on. Since some of the algorithms we will apply do not accept missing values and we would like to compared the algorithms based on their performance, we will remove the missing values. But before we perform the removal, we will make a number of additional variable transformations.

As the goal of our project is to compare public and private institutions, we want to combine the two types of private institutions : private for-profit and private non-profit into one group called private institutions. We use the `ifelse` command to achieve this by assigning level 0 ( private) to those institutions with a value of 2 (private for-profit) or 3(private non-profit) and level 1 to public institutions. 

```{r,message=FALSE, warning=FALSE}

csb.var.df %>%
  mutate(CONTROL = ifelse((CONTROL==2|CONTROL==3),"Public","Private")) %>%
  mutate(CONTROL    = as.factor(CONTROL)) %>%
  {.} ->csb.new.df
```

The data frame `csb.new.df` now contains the mutated variable `CONTROL` with only two levels - 0 for private institutions and 1 for public institutions.

The variables `NTP4_PRIV` and `NTP4_PUB` describes the average net price of either Public or Private colleges, and are complementing each other. For this reason, we will built a new variable `NPT4` which describes the average net price for both public and private colleges.

To do this, we first define a function `replace_na_values` which takes the variable as the input and by looping over the entire length of the variable, replaces each NA values by 0.  We call this function on our two variables `NPT4_PRIV` and `NPT4_PUB`. Then we create a new variable `NPT4` by adding the two variables `NPT4_PRIV` and `NPT4_PUB` using the `MUTATE` command.

```{r, cache=TRUE,message=FALSE, warning=FALSE}
  replace_na_values <- function (variable){
    for (i in 1:length(variable)){
      if (is.na(variable[i])){
        variable[i] = 0
      }
    }
    return (variable)
  }

csb.new.df %>% 
  mutate(NPT4 = replace_na_values(NPT4_PRIV) + replace_na_values(NPT4_PUB)) %>%
  {.} -> csb.new.df

csb.new.df %>% select(-NPT4_PRIV, -NPT4_PUB) %>% {.} -> csb.new.df
```

After we have performed the transformation of some of the variables, we will remove the observations with missing values.

```{r}
require(IDPmisc)

csb.omit.na.df = NaRV.omit(csb.new.df)
```

Since the names of these variables are not easy to understand, we rename them using the `rename` function of the `dplyr` package.

```{r,message=FALSE, warning=FALSE}
csb.omit.na.df %>%
  rename(Type = CONTROL,
         Family_Inc = FAMINC,
         Female_perc = FEMALE,
         Married_perc = MARRIED,
         Fin_Indep_perc = DEP_STAT_PCT_IND,
         First_Gen_perc = FIRST_GEN,
         Cost = NPT4,
         Num_Branches = NUMBRANCH, 
         Agric_deg_perc = PCIP01,
         Nature_deg_perc = PCIP03,
         Architect_deg_perc = PCIP04,
         Cultural_deg_perc = PCIP05,
         Communication_deg_perc = PCIP09,
         Technology_deg_perc= PCIP10, 
         Cohort_def_rate = CDR3, 
         Num_students_cohort = CDR3_DENOM, 
         Pell_grant_perc = PCTPELL, 
         Deg_loan_perc = PCTFLOAN,
         Num_stud_median_debt =  DEBT_N, 
         Num_stud_cum_debt = CUML_DEBT_N) %>%
         {.} -> csb.ren.df
```
The data frame `csb.ren.df` contains our final set of variables, ready for further analysis.

### 5.3 Creating training and test datasets

To split the dataset into training and test sets, we first set a see so that we can get the same sequence of random numbers whenever we supply the same seed in the random number generator.

```{r,message=FALSE, warning=FALSE}
set.seed(123)
```

Then we use the `sample.split()` fucntion which is part of `caTools` package. The split ratio is 0.8 which implies that 80% of observations go in the train set and the the rest 20% in the test set. The code below will create a new column `new` in the dataset that will have the 80% of rows having TRUE values and 20% with FALSE values.

```{r,message=FALSE, warning=FALSE}
csb.ren.df$new=sample.split(csb.ren.df,SplitRatio=0.8)
```

Next we create a `train` dataframe which is a subset of the original dataframe `csb.ren.df` containing only those observations that have TRUE values for the new column.

```{r,message=FALSE, warning=FALSE}
train=subset(csb.ren.df, csb.ren.df$new==TRUE)
```

We also create a `test` dataframe which is a subset of the original dataframe `csb.ren.df` containing only those observations that have FALSE values for the new column.

```{r,message=FALSE, warning=FALSE}
test=subset(csb.ren.df, csb.ren.df$new==FALSE)
```

In the next step, we drop the `new` column from the `train` and `test` dataframes.

```{r,message=FALSE, warning=FALSE}
train %>%
  select(-new) %>%
{.} -> train
```

```{r,message=FALSE, warning=FALSE}
test %>%
  select(-new) %>%
{.} -> test
```

Finally, we have the `train` and `test` dataframes ready for fitting the models.

# 6. Decision Trees

The following libraries are loaded for fitting the decision tree.

```{r,message=FALSE, warning=FALSE}
library(rpart)
library(rpart.plot)
library(rattle)
```

### 6.1 Building the tree

The code below uses the `rpart` function to fit a decision tree to the `train` dataframe, with `Type` as the categorical target variable and all other variables as the predictors, using the default control parameters.

```{r,message=FALSE, warning=FALSE}
treefit <- rpart(Type ~ . - Type, data = train, method = "class", control = rpart.control())
```

Next we display the results for the classification tree.
```{r,message=FALSE, warning=FALSE}
print(treefit)
```

```{r,message=FALSE, warning=FALSE}
printcp(treefit)
```

The `cptable` above provides a brief summary of the overall fit of the model. The table is printed from the smallest tree (no splits) to the largest one (10 splits). Looking at the table, we see that the best tree has 10 splits with the lowest cp of 0.01 based on cross-validation with the smalles cross-validation error of 0.24601. We retrieve this value from the table.

```{r,message=FALSE, warning=FALSE}
bestcp <- treefit$cptable[which.min(treefit$cptable[,"xerror"]), "CP"]
```

### Pruning the tree
Now we prune the tree using this "best" complexity parameter.

```{r,message=FALSE, warning=FALSE}
fit.pruned <- prune(treefit, cp = bestcp)
```

```{r,message=FALSE, warning=FALSE}
prp(fit.pruned)
```

### Predicting on Test dataset

To assess the performance of the classification tree, we predict on the `test` dataframe using the `predict` fucntion and saving the output in `out` dataframe.

```{r,message=FALSE, warning=FALSE}
out <- predict(fit.pruned, test)
```

Then we look at the first six rows of the `out` dataframe.
```{r,message=FALSE, warning=FALSE}
head(out)
```

The output tells us that each row contains the predcited probabilities for each value of the target variable `Type`. For instance, the we can see that predicted probability for the first observation to be classified as a Private college is 0.9651 and to be a Public college is 0.03482. For each observation, we'll need to find which class has the highest probability and then assign that class to the observation.

To do this, we pass `which.max` to apply, with a second argument of 1 to indicate we want to apply the function to each row.

```{r,message=FALSE, warning=FALSE}
confusion= table(apply(out,1,which.max),test$Type)
confusion
```

The output above shows the confusion matrix for the classification tree. From the output we see that 265 Private colleges and 529 Public colleges were classified correctly. The code below is used to calculate the misclassification rate. 
```{r,message=FALSE, warning=FALSE}
sum(confusion[row(confusion) != col(confusion)]) / sum(confusion)
```

The output tells us that the 7.67% of the observations were predicted incorrectly. To reduce the misclassification rate, we tune the hyperparameters of the model.

## Changing control parameters

We change the default parameters.

```{r,message=FALSE, warning=FALSE}
treefit1 <- rpart(Type ~ . - Type, data = train, control = rpart.control(minsplit=10,cval=15,cp=0, minbucket=20))
out1 <- predict(treefit1, test)
confusion_1 = table(apply(out1,1,which.max),test$Type)
sum(confusion_1[row(confusion_1) != col(confusion_1)]) / sum(confusion_1)
```

On changing the parameters, we are able to successfully reduce the misclassificate rate from 7.67 % to 6.98 %.

# 7. Decision Trees - Part II
We would like to modify the hyperparameters of decision tree model further to tune the model more specifically.

### 7.1 Building the tree

The code below uses the `rpart` function to fit a decision tree to the `train` dataframe, with `Type` as the categorical target variable and all other variables as the predictors. For this time, we start by using the arguments `minsplit=0` and `cp=0` as the setting of the `control` paramters.

```{r,message=FALSE, warning=FALSE}
tree2fit <- rpart(Type ~ . - Type, data = train, method = "class", control = rpart.control(minsplit=0, cp=0))
```

Next we select the appropriate CP value by choosing the lowest level with the minumum `xerror` value.
```{r,message=FALSE, warning=FALSE}
printcp(tree2fit)
```

The `cptable` above provides a brief summary of the overall fit of the model. The table is printed from the smallest tree (no splits) to the largest one (26 splits). Looking at the table, we see that the best tree has 17 splits with the cp value of 0.00159744 with the smallest cross-validation error of 0.17252. 

We could also plot the cp value with X-val Relative Error as below: 
```{r,message=FALSE, warning=FALSE}
plotcp(tree2fit)
```

As we can see, the `xerror` drops dramatically till the ninth split and then decrease slowly till the twentieth split.

As follow, We need to retrieve the `cp` value with the lowest `xerror`  from the table:

```{r,message=FALSE, warning=FALSE}
bestcp <-tree2fit$cptable[which.min(tree2fit$cptable[,"xerror"]), "CP"]
```

### Pruning the tree
Now we prune the tree using this "best" complexity parameter.

```{r,message=FALSE, warning=FALSE}
fit.pruned <- prune(tree2fit, cp = bestcp)
```

```{r,message=FALSE, warning=FALSE}
prp(fit.pruned)
```

### Predicting on Test dataset

To assess the performance of the classification tree, we predict on the `test` dataframe using the `predict` fucntion and saving the output in `out` dataframe.

```{r,message=FALSE, warning=FALSE}
out <- predict(fit.pruned, test)
```

Then we look at the first six rows of the `out` dataframe.
```{r,message=FALSE, warning=FALSE}
head(out)
```

The output tells us that each row contains the predcited probabilities for each value of the target variable `Type`. For instance, the we can see that predicted probability for the first observation to be classified as a Private college is 0.97848101 and to be a Public college is 0.02151899. For each observation, we'll need to find which class has the highest probability and then assign that class to the observation.

To do this, we pass `which.max` to apply, with a second argument of 1 to indicate we want to apply the function to each row.

```{r,message=FALSE, warning=FALSE}
confusion= table(apply(out,1,which.max),test$Type)
confusion
```

The output above shows the confusion matrix for the classification tree. From the output we see that 278 Private colleges and 527 Public colleges were classified correctly. The code below is used to calculate the misclassification rate. 
```{r,message=FALSE, warning=FALSE}
sum(confusion[row(confusion) != col(confusion)]) / sum(confusion)
```

The output tells us that the 6.40% of the observations were predicted incorrectly. 

After we have modified the hyperparameters of decision tree model, the misclassification rate on the test set drops from 7.67% to 6.40%. The model performance increases a little bit after this hyperparameters modification process. 

# 8 K-nearest Neighbors
The following libarires are loaded. 
```{r}
library(kknn)
library(caret)
library(e1071)
```
The k-nearest neighbors model is built using training data set. 
```{r}
kneighbors<-train.kknn(Type ~ ., data=train, kmax = 9)
kneighbors
```
The result above indicates that the best k=8 with the missclassification rate as 0.064. 

The prediction is made on test dataset from k-nearet neighbors model. 
```{r}
kprediction<-predict(kneighbors,test)
kprediction
```

The display details on the correctness of the prediction are shown below:
```{r}
confusionMatrix(reference=test$Type, data=kprediction)
```
The results above indicate that the prediction accuracy is 0.922. The sensitivity is 0.89 and the specificity is around 0.94. The positive class is Private school. 89% of the private school can be predicted correctly.94% of the Public school can be predicted correctly. The prediction result is very good. 

The relationship between missclassification and the number of k is shown below. 
```{r}
plot(kneighbors)
```
The result above indicates that when k is at least 8, the misclassification is smallest, around 0.064. When k is less than 4, the misclassification is around 0.076; when k is between 5 and 7, the misclassification is around 0.068. To have least misclassification rate, k is at least 8. The larger the k, the less the noise is. Thus, k=8 is a good number to use in the nearest neighbors model. 

# K-nearest Neighbors - Part 2

The `caret` package makes available the `confusionMatrix` function
([documentation](https://artax.karlin.mff.cuni.cz/r-help/library/caret/html/confusionMatrix.html).)
The `kknn` package makes available the `kknn` function.
The `e1071` package is required by the `caret` package.
```{r message=FALSE}
library(kknn)
library(caret)
library(e1071)
```

We are reading and displaying the first 10 rows from the data set after the variable transformations. We can see that 
```{r}
Data <- csb.ren.df
head(Data, 10)
dim(Data)
```


We can see that the variables are with different scale. Since we are performing the k-neightbours algorithm, the distance which is calculated will determine the final prediction. If we have variables with different scale, the ones which have higher scale will influence the distance calculation in a stronger way. In order to avoid scale influences on the distance calculation, we will perform scaling of the variables.

```{r}
scaled.data <- scale(data.frame(Data[,-1 ]))
scaled.data <- cbind(scaled.data, Data[1])

# check that we get mean of 0 and sd of 1
#colMeans(scaled.data)  # faster version of apply(scaled.dat, 2, mean)
#apply(scaled.data, 2, sd)
```

After the data variables are scaled, the next step is to split the data into a training and testing data frame. We will pick 50 of the observations for the testing data set and will leave the rest for the training data set.
```{r}
#Sample   <- sample(1:150, 50)
#testing  <- data.frame(scaled.data[ Sample, ])
#training <- data.frame(scaled.data[-Sample, ])
dim(train)
dim(test)
```

At this point we are ready to run the k-neighbours model and get predictions for the testing data set. We will first perform the kknn algorithm by using the default parametrization. We will look into predicting based on 9 maximum number of neighbors.

```{r}
suppressWarnings(suppressMessages(library(kknn)))
model <- train.kknn(Type ~ ., 
                    data = train, 
                    kmax = 9,
                    response="ordinal")
model
```


We can see that with this model, the best results with the minal missclassification happen when the maximum number of neighbours (9) is used. We will apply the predict function on the testing data set and the results are shown below.

```{r}
prediction <- predict(model, test)
prediction
```

We will use the predicted results with the confusion matrix in order to derive the accuracy of the model.

```{r}
confusionMatrix(test$Type, prediction)
#table(testing[,1], prediction)
```
We have achieved an accuracy of 92%. The sensitivity (or the proportion of positives that are correctly identified) is 96.3% and the specificity (the proportions of negatives that are correctly identified) is 86.9%. 

When plotting the missclassification agains the number of the neighbours used we can see that with the increase in the number of neighbours, there is a decrease in the missclassification.

```{r}
plot(model)
```


The k-neighbors algorithm from the train.kknn package allows us to explore how using different kernels and different values of the kmax parameter affects the results. We are also able to see from the results which values are optimal for achieving minimal missqualification error.


```{r}
suppressWarnings(suppressMessages(library(kknn)))
model <- train.kknn(Type ~ ., 
                    data = train, 
                    kernel = c("rectangular", "triangular", "epanechnikov", "gaussian", 
 	"rank", "optimal", "biweight","triweight", "cos", "inv", "optimal"),
                    kmax = 40)
model
```

We can see that we get the minimum classification error when kernel = "biweight" and the maximum number of k is equal to 29.

```{r}
prediction <- predict(model, test)
confusionMatrix(test$Type, prediction)
#table(testing[,1], prediction)
```
We have achieved an accuracy of 96%. The sensitivity (or the proportion of positives that are correctly identified) is 100% and the specificity (the proportions of negatives that are correctly identified) is 93.1%. 


In addition to both prameters we already discussed: kernel and the maximum number of neighbours(kmax), another parameter to explore further is the Minkowski distance. By default the distance is set to 2. We will explore how the best solution, which achieves the minimal missclassification, will change when we select a different distance. First, we will consider the case when distance is 1.


```{r}
suppressWarnings(suppressMessages(library(kknn)))
model <- train.kknn(Type ~ ., 
                    data = train, 
                    kernel = c("rectangular", "triangular", "epanechnikov", "gaussian", 
 	"rank", "optimal", "biweight","triweight", "cos", "inv", "optimal"),
                    kmax = 40,
 	                  distance = 1)
model
plot(model)
```

We can see that in this case we get the minimum classification error when kernel = "cos" and the maximum number of k is equal to 20. We will check the confusion matrix for this solution.


```{r}
prediction <- predict(model, test)
confusionMatrix(test$Type, prediction)
#table(testing[,1], prediction)
```
We have achieved an accuracy of 94%. The sensitivity (or the proportion of positives that are correctly identified) is 100% and the specificity (the proportions of negatives that are correctly identified) is 93.1%. 


As a next step, we will continue exploring how changing the Markowski distance will affect the final result. We will change the Markowski distance to 3.

```{r}
suppressWarnings(suppressMessages(library(kknn)))
model <- train.kknn(Type ~ ., 
                    data = train, 
                    kernel = c("rectangular", "triangular", "epanechnikov", "gaussian", 
 	"rank", "optimal", "biweight","triweight", "cos", "inv", "optimal"),
                    kmax = 40,
 	                  distance = 3)
model
plot(model)
```

We can see that in this case we get the minimum classification error when kernel = "triweight" and the maximum number of k is equal to 28. We will check the confusion matrix for this solution.


```{r}
prediction <- predict(model, test)
confusionMatrix(test$Type, prediction)
#table(testing[,1], prediction)
```

Since the Minkowski distance determines the shape which is formed by the neighbours, it is important to cover values which are smaller than 1.

We will explore next a distance which is equal to 0.35.


```{r}
suppressWarnings(suppressMessages(library(kknn)))
model <- train.kknn(Type ~ ., 
                    data = train, 
                    kernel = c("rectangular", "triangular", "epanechnikov", "gaussian", 
 	"rank", "optimal", "biweight","triweight", "cos", "inv", "optimal"),
                    kmax = 40,
 	                  distance = 0.35)
model
plot(model)
```

We can see that in this case we get the minimum classification error when kernel = "nominal" and the maximum number of k is equal to 8. We will check the confusion matrix for this solution.

```{r}
prediction <- predict(model, test)
confusionMatrix(test$Type, prediction)
#table(testing[,1], prediction)
```

We have achieved an accuracy of 94%. The sensitivity (or the proportion of positives that are correctly identified) is 100% and the specificity (the proportions of negatives that are correctly identified) is 90%.

Now we will explore what the results are when the distance is 0.7.


```{r}
suppressWarnings(suppressMessages(library(kknn)))
model <- train.kknn(Type ~ ., 
                    data = train, 
                    kernel = c("rectangular", "triangular", "epanechnikov", "gaussian", 
 	"rank", "optimal", "biweight","triweight", "cos", "inv", "optimal"),
                    kmax = 40,
 	                  distance = 0.7)
model
plot(model)
```

We can see that in this case we get the minimum classification error when kernel = "nominal" and the maximum number of k is equal to 27. We will check the confusion matrix for this solution.


```{r}
prediction <- predict(model, test)
confusionMatrix(test$Type, prediction)
#table(testing[,1], prediction)
```

We have achieved an accuracy of 96%. The sensitivity (or the proportion of positives that are correctly identified) is 100% and the specificity (the proportions of negatives that are correctly identified) is 93.1%. 

We can conclude that the best results are achieved in two cases. First when the kernel = "biweight", the maximum number of k is equal to 29 and distance is 2. And the second case is when the kernel = "nominal", the maximum number of k is equal to 27 and the Markowski distance is 0.7.

